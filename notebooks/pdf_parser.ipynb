{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BERT based NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text('../data/ashok_leyland_concall_transcript.pdf')\n",
    "# text1 = extract_text('../data/AGM_Transcript_Revised.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A.V. Mani Sundaram'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Idea -> Creating tokens as Points of Interest (POI) in the text extracted from the PDF\n",
    "# and using these tokens to extract Entities of Interest (EOI)\n",
    "string = 'I now invite Mr. A.V. Mani Sundaram, CLID IN30163741521740.<mask>A.V. Mani Sundaram:'\n",
    "rege = r'(<mask>(.*)<mask>(.*?):)|(<mask>(.*?):)'\n",
    "\n",
    "re.findall(rege, string)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: The processed data's accuracy will depend on the quality of pdf extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Creating POI\n",
    "text = text.replace('\\n\\n', '<mask>').replace('\\n', '<m>').replace('\\x0c', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extracting EOI\n",
    "a = re.findall(r'<mask>(.*?):', text)\n",
    "a = [str(_.strip()) + ':' for _ in a]\n",
    "\n",
    "final_list = []\n",
    "for string in a:\n",
    "    match_group = re.findall(r'(<mask>(.*)<mask>(.*?):)|(<mask>(.*?):)', string)\n",
    "    non_empty_group = [ele for inner_group in match_group for ele in inner_group if ele]\n",
    "    # print(non_empty_group)\n",
    "    if match_group:\n",
    "        if len(non_empty_group[0]) < 2:\n",
    "            pass\n",
    "        else:\n",
    "#             print(match_group[0][1])\n",
    "            final_list.append(non_empty_group[-1])\n",
    "#     final_list.extend(re.findall(r'<mask><mask>(.*)<mask><mask>(.*?):', string)[-1])\n",
    "\n",
    "# Approach: Weed out entities based on length of the string\n",
    "# Assumption: A typical name would not exceed 40 characters\n",
    "filtered_list = [filtered_element for filtered_element in final_list if len(filtered_element) <= 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tentative Approach: Trying to filter out PER entities  (discarded)\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "for each_element in filtered_list:\n",
    "    ner = nlp(each_element.replace('.', ''))\n",
    "#     print(ner)\n",
    "# Cons: Does not work as expected; Moderator, Management does not fit under any entity; breaks down single words\n",
    "# and provides entities (is this expected?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 3: Extracting location of all EOI\n",
    "entity_span = []\n",
    "for element in filtered_list:\n",
    "    for find_result in re.finditer(str(element) + ':', text):\n",
    "        span = (element, int(find_result.start()), int(find_result.end()))\n",
    "        entity_span.append(span)\n",
    "entity_span = list(set(entity_span))\n",
    "entity_span = sorted(entity_span, key=lambda span_list: span_list[1])\n",
    "# print(entity_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extracting relevant information based on the EOI\n",
    "\n",
    "desired_columns = [\"Sr.No.\", \"Name\", \"GroupOfSentences\"]\n",
    "information_frame = pd.DataFrame(None, columns=desired_columns)\n",
    "for sequence, entity_information in enumerate(entity_span):\n",
    "    start_index = entity_information[1]\n",
    "    end_index = entity_information[2]\n",
    "    if entity_information[0].lower() == \"management\":\n",
    "        management_string = text[\n",
    "            end_index: entity_span[sequence+1][1]\n",
    "        ].replace(\"<mask>\", \"|\").replace(\"<m>\", \"|\")\n",
    "        management_list = management_string.split(\"|\")\n",
    "        \n",
    "        frame = pd.DataFrame([[sequence, entity_information[0], management_string]], columns=desired_columns)\n",
    "\n",
    "    elif (\"directors\" in entity_information[0].lower()) and (\"board\" in entity_information[0].lower()):\n",
    "        director_string = text[\n",
    "            end_index: entity_span[sequence+1][1]\n",
    "        ].replace(\"<mask>\", \"|\").replace(\"<m>\", \"|\")\n",
    "        director_list = director_string.split(\"|\")\n",
    "\n",
    "        frame = pd.DataFrame([[sequence, entity_information[0], director_string]], columns=desired_columns)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            relevant_string = text[\n",
    "                end_index: entity_span[sequence+1][1]\n",
    "            ].replace(\"<mask>\", \" \").replace(\"<m>\", \" \")\n",
    "        except IndexError:\n",
    "            relevant_string = text[\n",
    "                end_index:\n",
    "            ].replace(\"<mask>\", \" \").replace(\"<m>\", \" \")\n",
    "    \n",
    "        frame = pd.DataFrame([[sequence, entity_information[0], relevant_string]], columns=desired_columns)\n",
    "    \n",
    "    information_frame = pd.concat([information_frame, frame], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information_frame.to_excel(\"../data/ashok_leyland_concall_transcript.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12466f790b01845891e8c54f2878097070b87efd364ef8a7c7054cc189b73841"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('con_summary_env': venv)",
   "language": "python",
   "name": "python391jvsc74a57bd012466f790b01845891e8c54f2878097070b87efd364ef8a7c7054cc189b73841"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
