{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Extract_BERT_Embeddings.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNc+kFZ5lt0XA54aT4VCSas"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CAUJS8dwS_pn"},"source":["**Understanding BERT embeddings based on the code shared by Chris McCormick**\n","\n","https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n"]},{"cell_type":"code","metadata":{"id":"M1aGpumHEn5f","executionInfo":{"status":"ok","timestamp":1635313725750,"user_tz":-330,"elapsed":8252,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}}},"source":["!pip install -q transformers\n","!pip install -q sentence_transformers"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"05UlCDSsDcxy","executionInfo":{"status":"ok","timestamp":1635313738522,"user_tz":-330,"elapsed":12775,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}}},"source":["import torch\n","from transformers import BertTokenizerFast, BertModel # Use BertTokenizerFast to ensure that words_ids() can be used\n","from sentence_transformers import util"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3zM96LkWC1C","executionInfo":{"status":"ok","timestamp":1635313738523,"user_tz":-330,"elapsed":19,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}}},"source":["import numpy as np\n","import nltk"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8EKPIwL4RTFJ","executionInfo":{"status":"ok","timestamp":1635313738525,"user_tz":-330,"elapsed":19,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"1774dcd1-e14f-4eda-840f-013c7b9677c5"},"source":["nltk.download('punkt')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KS1nsUpHD6By","executionInfo":{"status":"ok","timestamp":1635313745071,"user_tz":-330,"elapsed":6556,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"bc602ddc-7f82-4aa6-d27f-9de6d9a97065"},"source":["# Initialize the tokenizer and model\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n","\n","model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n","model.eval()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"HVfO72jKW27u"},"source":["#### Step 1: Map a word of interest to the set of tokens corersponding to it"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZTTP-32eHmtj","executionInfo":{"status":"ok","timestamp":1635313745071,"user_tz":-330,"elapsed":42,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"951be855-268f-4090-b8ec-bfc5d5de330e"},"source":["# Scenario 1: A word with a punctuation is split into multiple tokens and \n","# these tokens are treated as separate words\n","trial_sent_1 = \"I like cookie's\"\n","\n","current_token_list = tokenizer.tokenize(trial_sent_1)\n","print(current_token_list)\n","current_encoding = tokenizer.encode_plus(trial_sent_1)\n","print(current_encoding)\n","current_encoding.word_ids()"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'like', 'cookie', \"'\", 's']\n","{'input_ids': [101, 146, 1176, 25413, 112, 188, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"]},{"output_type":"execute_result","data":{"text/plain":["[None, 0, 1, 2, 3, 4, None]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ala53X5yXOhs","executionInfo":{"status":"ok","timestamp":1635313745072,"user_tz":-330,"elapsed":31,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"936bd136-5cb7-4c12-8e00-d424ccc86bd0"},"source":["# Scenario 2: A word without a punctuation is split into multiple tokens and \n","# these tokens are treated as a single word\n","trial_sent_2 = \"I like cookys\"\n","\n","current_token_list = tokenizer.tokenize(trial_sent_2)\n","print(current_token_list)\n","current_encoding = tokenizer.encode_plus(trial_sent_2)\n","current_encoding.word_ids()"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'like', 'cook', '##ys']\n"]},{"output_type":"execute_result","data":{"text/plain":["[None, 0, 1, 2, 2, None]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3ajzw4TGiYF","executionInfo":{"status":"ok","timestamp":1635313745073,"user_tz":-330,"elapsed":29,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"81469982-b82c-487e-c0f6-f53bc788c929"},"source":["# Scenario 3: Index of 'word of interest' needs to be identified using the location of the word or \n","# subwords post tokenization\n","trial_sent_3 = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank\"\n","\n","current_token_list = tokenizer.tokenize(trial_sent_3)\n","current_token_list"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['After',\n"," 'stealing',\n"," 'money',\n"," 'from',\n"," 'the',\n"," 'bank',\n"," 'vault',\n"," ',',\n"," 'the',\n"," 'bank',\n"," 'r',\n"," '##ob',\n"," '##ber',\n"," 'was',\n"," 'seen',\n"," 'fishing',\n"," 'on',\n"," 'the',\n"," 'Mississippi',\n"," 'river',\n"," 'bank']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7ijZUNlHXXS","executionInfo":{"status":"ok","timestamp":1635313745075,"user_tz":-330,"elapsed":27,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"a7e02481-e16d-48ea-96bf-244d226d22f6"},"source":["# Python's default string split is different from that obtained using BERT tokenizer\n","# Evaluate the use NLTK's tokenizer\n","trial_sent_3.split(' ')"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['After',\n"," 'stealing',\n"," 'money',\n"," 'from',\n"," 'the',\n"," 'bank',\n"," 'vault,',\n"," 'the',\n"," 'bank',\n"," 'robber',\n"," 'was',\n"," 'seen',\n"," 'fishing',\n"," 'on',\n"," 'the',\n"," 'Mississippi',\n"," 'river',\n"," 'bank']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQgb5i_zKemw","executionInfo":{"status":"ok","timestamp":1635313745076,"user_tz":-330,"elapsed":25,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"3005fbc8-7fdb-45c0-cb19-6e81328604ca"},"source":["nltk.tokenize.word_tokenize(trial_sent_3)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['After',\n"," 'stealing',\n"," 'money',\n"," 'from',\n"," 'the',\n"," 'bank',\n"," 'vault',\n"," ',',\n"," 'the',\n"," 'bank',\n"," 'robber',\n"," 'was',\n"," 'seen',\n"," 'fishing',\n"," 'on',\n"," 'the',\n"," 'Mississippi',\n"," 'river',\n"," 'bank']"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"_YQI5UsiZiPg"},"source":["#### Step 2: Obtain token embeddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0QSudfGZ32e","executionInfo":{"status":"ok","timestamp":1635313745076,"user_tz":-330,"elapsed":22,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"7def1ec6-ba42-4894-fb3c-266091b70c26"},"source":["actual_sent = \"I love cookies\"\n","current_token_list = tokenizer.tokenize(actual_sent)\n","print(current_token_list)\n","\n","current_encoding = tokenizer.encode_plus(actual_sent)\n","print(current_encoding)\n","current_encoding.word_ids()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'love', 'cookies']\n","{'input_ids': [101, 146, 1567, 18621, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n"]},{"output_type":"execute_result","data":{"text/plain":["[None, 0, 1, 2, None]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"sERv9y0obF4J","executionInfo":{"status":"ok","timestamp":1635313745077,"user_tz":-330,"elapsed":20,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}}},"source":["indexed_tokens = current_encoding['input_ids']\n","segments_ids = [1] * len(indexed_tokens)\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dEVM06WbptN","executionInfo":{"status":"ok","timestamp":1635313745078,"user_tz":-330,"elapsed":20,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"7b1c45ad-c71b-44be-9870-579077f561da"},"source":["print(tokens_tensor.size())\n","segments_tensors.size()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 5])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"5OBr_R2CY6t_","executionInfo":{"status":"ok","timestamp":1635313745079,"user_tz":-330,"elapsed":18,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}}},"source":["with torch.no_grad():\n","  outputs = model(tokens_tensor, segments_tensors)\n","  hidden_states = outputs[2]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DdvmGZUbj-e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635313745738,"user_tz":-330,"elapsed":676,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"95db2890-40a7-41bf-a4ac-b072d709e603"},"source":["# Tuple length = number of hidden layers (default = 12) + token embeddings (i.e. embeddings corresponding to the input layer)\n","len(hidden_states)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BaSm0TyrpUX","executionInfo":{"status":"ok","timestamp":1635313745746,"user_tz":-330,"elapsed":47,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"1e2caa4d-cefb-4264-a1c8-79cd1f5ac612"},"source":["# For a given layer, a tensor of size Batch x Number_Of_Tokens (including [CLS] and [SEP]) x Dimension\n","hidden_states[0].size()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PblyROiEsczG","executionInfo":{"status":"ok","timestamp":1635313745747,"user_tz":-330,"elapsed":45,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"d958d728-9a4f-4724-bd31-19ceeaf749c6"},"source":["token_embeddings = torch.stack(hidden_states, dim=0)\n","token_embeddings.size()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 1, 5, 768])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kiiX-EhEvJ7S","executionInfo":{"status":"ok","timestamp":1635313745747,"user_tz":-330,"elapsed":42,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"63a76888-47a1-4e94-c960-82cfa7bf8155"},"source":["# Remove dimension 1 i.e. \"batches\"\n","token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","token_embeddings.size()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 5, 768])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_A1NEXGSvMWF","executionInfo":{"status":"ok","timestamp":1635313745748,"user_tz":-330,"elapsed":40,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"38299240-6562-4b4f-96ef-74f98e4031f2"},"source":["# Swap dimensions 0 and 1.\n","token_embeddings = token_embeddings.permute(1,0,2)\n","token_embeddings.size()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 13, 768])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gIADSTn6vXO0","executionInfo":{"status":"ok","timestamp":1635313745749,"user_tz":-330,"elapsed":40,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"769d7711-08fd-4d7c-f01b-3b4de47e2461"},"source":["# Pooling strategy -- sum together the last four layers\n","token_vecs_sum = []\n","\n","# For each token in the sentence...\n","for token in token_embeddings:\n","    sum_vec = torch.sum(token[-4:], dim=0) # returns column sum\n","    token_vecs_sum.append(sum_vec)\n","\n","print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape is: 5 x 768\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2BN3SxOtLRW1","executionInfo":{"status":"ok","timestamp":1635313745749,"user_tz":-330,"elapsed":37,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"2d4fc499-c71c-439a-b13d-224f62ef7bea"},"source":["# Direct implementation of the pooling strategy\n","rev_token_embeddings = token_embeddings.detach().clone()\n","rev_token_embeddings = rev_token_embeddings[:, -4:, :]\n","print(rev_token_embeddings.size())\n","\n","rev_token_sum = torch.sum(rev_token_embeddings, dim=1)\n","rev_token_sum.size()"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 4, 768])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 768])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAfb_lUCOcO_","executionInfo":{"status":"ok","timestamp":1635313745750,"user_tz":-330,"elapsed":34,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"b7ec901e-cee4-4d40-ab06-79bee28e9692"},"source":["torch.equal(rev_token_sum[0], token_vecs_sum[0])"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"_h0AzFVt-fA4"},"source":["#### Step 3: Obtain the word embedding by pooling over the relevant token embeddings\n"]},{"cell_type":"code","metadata":{"id":"Jn9lBfew4VrX","executionInfo":{"status":"ok","timestamp":1635313745750,"user_tz":-330,"elapsed":31,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}}},"source":["# If multiple tokens map to a single word, then obtain the mean of the relevant token embeddings\n","# sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L48v67r4HJJ0"},"source":["#### Step 4: Understand the contextual difference in the use of the word \"bank\""]},{"cell_type":"code","metadata":{"id":"4qaUMcQSHSQS","executionInfo":{"status":"ok","timestamp":1635313745751,"user_tz":-330,"elapsed":32,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}}},"source":["multiple_context_sent = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank\"\n","\n","current_encoding = tokenizer.encode_plus(multiple_context_sent)\n","indexed_tokens = current_encoding['input_ids']\n","segments_ids = [1] * len(indexed_tokens)\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])\n","\n","with torch.no_grad():\n","  outputs = model(tokens_tensor, segments_tensors)\n","  hidden_states = outputs[2]\n","\n","token_embeddings = torch.stack(hidden_states, dim=0)\n","token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","token_embeddings = token_embeddings.permute(1,0,2)\n","\n","# Pooling strategy -- sum together the last four layers\n","token_vecs_sum = []\n","\n","# For each token in the sentence...\n","for token in token_embeddings:\n","    sum_vec = torch.sum(token[-4:], dim=0) # returns column sum\n","    token_vecs_sum.append(sum_vec)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s4u1iJ_RJomC","executionInfo":{"status":"ok","timestamp":1635313745752,"user_tz":-330,"elapsed":32,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"c1a70977-192f-4222-ca95-cf6fbfad467f"},"source":["# Add 1 to the values below as [CLS] token is not considered\n","np.where(np.array(tokenizer.tokenize(multiple_context_sent)) == 'bank')[0]"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 5,  9, 20])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zeYy8lpXRLt4","executionInfo":{"status":"ok","timestamp":1635313745752,"user_tz":-330,"elapsed":30,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"24e3a91e-7326-413b-ae3d-58199afbe696"},"source":["# Each occurence of the word \"bank\" has the same index\n","for idx in [6, 10, 21]:\n","  print(indexed_tokens[idx])"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["3085\n","3085\n","3085\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1lTBpMxKW_o","executionInfo":{"status":"ok","timestamp":1635313745752,"user_tz":-330,"elapsed":28,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"ee6afadf-6e31-442e-995a-5550c4bc036c"},"source":["# [CLS] + 21 + [SEP]\n","len(token_vecs_sum)"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["23"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QX5TVX2rK448","executionInfo":{"status":"ok","timestamp":1635313745753,"user_tz":-330,"elapsed":27,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"9daa039a-b20c-4bcc-ecef-4846be9212d8"},"source":["bank_tensors = torch.cat((torch.unsqueeze(token_vecs_sum[6], 0)\n","                          , torch.unsqueeze(token_vecs_sum[10], 0)\n","                          , torch.unsqueeze(token_vecs_sum[21], 0))\n","                          , dim=0)\n","bank_tensors.size()"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 768])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4RCgcN8M1S5","executionInfo":{"status":"ok","timestamp":1635313745753,"user_tz":-330,"elapsed":25,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"0c565a31-c164-4afb-9c29-5413427dd0ae"},"source":["# <bank vault, bank robber, river bank>\n","# Context changes the similarity between the same word\n","util.cos_sim(bank_tensors, bank_tensors)"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.9078, 0.5361],\n","        [0.9078, 1.0000, 0.5107],\n","        [0.5361, 0.5107, 1.0000]])"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"f9NOaB5BP6aa"},"source":["#### Section 4: Non-context specific word embedding"]},{"cell_type":"code","metadata":{"id":"4PrdNqfsQBXU","executionInfo":{"status":"ok","timestamp":1635313745754,"user_tz":-330,"elapsed":23,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}}},"source":["single_word_sent = \"bank\"\n","\n","current_encoding = tokenizer.encode_plus(single_word_sent)\n","indexed_tokens = current_encoding['input_ids']\n","segments_ids = [1] * len(indexed_tokens)\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])\n","\n","with torch.no_grad():\n","  outputs = model(tokens_tensor, segments_tensors)\n","  hidden_states = outputs[2]"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M09JSKjjQv3A","executionInfo":{"status":"ok","timestamp":1635313745754,"user_tz":-330,"elapsed":23,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"bb4a28bf-476d-4cf6-9579-a8693377a701"},"source":["# Three indexes as [CLS] + word + [SEP]\n","indexed_tokens"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101, 3085, 102]"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OScq7iVQQcIO","executionInfo":{"status":"ok","timestamp":1635313745754,"user_tz":-330,"elapsed":20,"user":{"displayName":"Vivek Jayaswal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07386092584236149418"}},"outputId":"79cebf85-f1cc-46ce-fb0b-b6fb2e4450c7"},"source":["token_embeddings = torch.stack(hidden_states, dim=0)\n","token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","token_embeddings = token_embeddings.permute(1,0,2)\n","token_embeddings.size()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 13, 768])"]},"metadata":{},"execution_count":32}]}]}